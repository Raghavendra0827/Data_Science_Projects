{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e49dd603",
   "metadata": {},
   "source": [
    "# 1. Regularization Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186d7534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3948535",
   "metadata": {},
   "source": [
    "### Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb046a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linearregression(X, y):\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(X, y, color='black', label='Data Points')\n",
    "    ax.plot(X, y_pred, color='blue', linewidth=3, label='Linear Regression')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_title('Linear Regression')\n",
    "    ax.legend()\n",
    "    st.pyplot(fig)\n",
    "\n",
    "    fig_3d = go.Figure()\n",
    "    fig_3d.add_trace(go.Scatter3d(x=X.squeeze(), y=y, z=y_pred, mode='markers', name='Actual Data Points'))\n",
    "    fig_3d.add_trace(go.Scatter3d(x=X.squeeze(), y=y_pred, z=y_pred, mode='lines', name='Predicted Line'))\n",
    "    fig_3d.update_layout(scene=dict(xaxis_title='X', yaxis_title='y', zaxis_title='Predicted y'))\n",
    "    st.write(\"## 3D Visualization - Linear Regression\")\n",
    "    st.plotly_chart(fig_3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec29268",
   "metadata": {},
   "source": [
    "### lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a337acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lasso(X, y, alpha):\n",
    "    lasso_model = Lasso(alpha=alpha)  \n",
    "    lasso_model.fit(X, y)\n",
    "    y_pred_lasso = lasso_model.predict(X)\n",
    "    coef = lasso_model.coef_[0]\n",
    "    intercept = lasso_model.intercept_\n",
    "    formula = f'y = {coef:.2f}X + {intercept:.2f}'\n",
    "    explanation = f\"In Lasso regression, the penalty term (alpha) is added to the absolute values of the coefficients (L1 regularization), which can result in sparse models with some coefficients being exactly zero.\"\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(X, y, color='black', label='Data Points')\n",
    "    ax.plot(X, y_pred_lasso, color='red', linewidth=2, label='Lasso Regression')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('y')\n",
    "    st.title(f'Lasso Regression')\n",
    "    st.write(f\"**Formula:** {formula}\")\n",
    "    st.write(f\"**Explanation:** {explanation}\")\n",
    "    ax.legend()\n",
    "    st.pyplot(fig)\n",
    "\n",
    "    fig_3d = go.Figure()\n",
    "    fig_3d.add_trace(go.Scatter3d(x=X.squeeze(), y=y, z=y_pred_lasso, mode='markers', name='Actual Data Points'))\n",
    "    fig_3d.add_trace(go.Scatter3d(x=X.squeeze(), y=y_pred_lasso, z=y_pred_lasso, mode='lines', name='Predicted Line'))\n",
    "    fig_3d.update_layout(scene=dict(xaxis_title='X', yaxis_title='y', zaxis_title='Predicted y'))\n",
    "    st.write(\"## 3D Visualization - Lasso Regression\")\n",
    "    st.plotly_chart(fig_3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cca397",
   "metadata": {},
   "source": [
    "### ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a9d226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge(X, y, alpha):\n",
    "    ridge_model = Ridge(alpha=alpha)  \n",
    "    ridge_model.fit(X, y)\n",
    "    y_pred_ridge = ridge_model.predict(X)\n",
    "    coef = ridge_model.coef_[0]\n",
    "    intercept = ridge_model.intercept_\n",
    "    formula = f'y = {coef:.2f}X + {intercept:.2f}'\n",
    "    explanation = f\"In Ridge regression, the penalty term (alpha) is added to the square of the coefficients (L2 regularization), which helps in reducing the complexity of the model.\"\n",
    "    color = 'green' if alpha < 1 else 'blue'  # Change color based on alpha value\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(X, y, color='black', label='Data Points')\n",
    "    ax.plot(X, y_pred_ridge, color=color, linewidth=2, label='Ridge Regression')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('y')\n",
    "    st.title(f'Ridge Regression')\n",
    "    st.write(f\"**Formula:** {formula}\")\n",
    "    st.write(f\"**Explanation:** {explanation}\")\n",
    "    ax.legend()\n",
    "    st.pyplot(fig)\n",
    "\n",
    "    fig_3d = go.Figure()\n",
    "    fig_3d.add_trace(go.Scatter3d(x=X.squeeze(), y=y, z=y_pred_ridge, mode='markers', name='Actual Data Points'))\n",
    "    fig_3d.add_trace(go.Scatter3d(x=X.squeeze(), y=y_pred_ridge, z=y_pred_ridge, mode='lines', name='Predicted Line'))\n",
    "    fig_3d.update_layout(scene=dict(xaxis_title='X', yaxis_title='y', zaxis_title='Predicted y'))\n",
    "    st.write(\"## 3D Visualization - Ridge Regression\")\n",
    "    st.plotly_chart(fig_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da982665",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "X = np.linspace(1, 10, 200).reshape(-1, 1) \n",
    "y = 2 * X.squeeze() + np.random.normal(0, 2, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ded690d",
   "metadata": {},
   "source": [
    "### Streamlit interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5259d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.sidebar.header('Regression Type')\n",
    "regression_type = st.sidebar.selectbox('Select Regression Type', ('Linear Regression', 'Lasso Regression', 'Ridge Regression'))\n",
    "\n",
    "if regression_type == 'Linear Regression':\n",
    "    Linearregression(X, y)\n",
    "elif regression_type == 'Lasso Regression':\n",
    "    alpha = st.sidebar.slider('Select Alpha', min_value=0.1, max_value=10.0, step=0.1)\n",
    "    lasso(X, y, alpha)\n",
    "else: # Ridge Regression\n",
    "    alpha = st.sidebar.slider('Select Alpha', min_value=0.1, max_value=100.0, step=0.5)\n",
    "    ridge(X, y, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327da8cc",
   "metadata": {},
   "source": [
    "#  2. Gradient Descent Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de48ac7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import streamlit as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e331167",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "num_samples = 100\n",
    "x1 = np.random.uniform(0, 10, num_samples)\n",
    "x2 = np.random.uniform(0, 10, num_samples)\n",
    "intercept = np.ones(num_samples)\n",
    "error_term = np.random.normal(0, 0.5, num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0470fec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept_true = 1.5\n",
    "coef1_true = 2\n",
    "coef2_true = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f53644",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = intercept_true * intercept + coef1_true * x1 + coef2_true * x2 + error_term\n",
    "feature_matrix = np.vstack([intercept, x1, x2]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ee3618",
   "metadata": {},
   "source": [
    "### True coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce66c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_coefficients = [intercept_true, coef1_true, coef2_true]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5720bbe4",
   "metadata": {},
   "source": [
    "### Gradient Descent Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc410d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_gradient_descent(learning_rate, iterations, initial_coefficients):\n",
    "    coefficients = initial_coefficients\n",
    "    coefficients_history = np.zeros((iterations + 1, 3))\n",
    "    coefficients_history[0, :] = initial_coefficients\n",
    "\n",
    "    loss_history = np.zeros(iterations)\n",
    "\n",
    "    for i in range(iterations):\n",
    "        predictions = np.dot(feature_matrix, coefficients)\n",
    "        residuals = y - predictions\n",
    "        gradient = -2 * np.dot(feature_matrix.T, residuals) / num_samples\n",
    "        coefficients = coefficients - learning_rate * gradient\n",
    "        coefficients_history[i + 1, :] = coefficients\n",
    "        loss_history[i] = np.mean(residuals ** 2)\n",
    "\n",
    "    return coefficients_history, loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c7f3f7",
   "metadata": {},
   "source": [
    "### Plotting Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98359fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_parameters_trajectory(coefficients_history, i, j, axis):\n",
    "    axis.plot(true_coefficients[i], true_coefficients[j], marker='p', markersize=10, color='red', label='True Coefficients')\n",
    "    axis.plot(coefficients_history[:, i], coefficients_history[:, j], linestyle='--', marker='o', markersize=5, label='Gradient Descent Path')\n",
    "    axis.set_xlabel(f'Coefficient {i}')\n",
    "    axis.set_ylabel(f'Coefficient {j}')\n",
    "    axis.legend()\n",
    "    axis.grid(True)\n",
    "\n",
    "def plot_gradient_descent(coefficients_history, loss_history, learning_rate, iterations):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "    fig.suptitle(f'Gradient Descent Visualization\\nLearning Rate: {learning_rate}, Iterations: {iterations}', fontsize=16)\n",
    "\n",
    "    plot_parameters_trajectory(coefficients_history, 0, 1, axes[0, 0])\n",
    "    plot_parameters_trajectory(coefficients_history, 0, 2, axes[0, 1])\n",
    "    plot_parameters_trajectory(coefficients_history, 1, 2, axes[1, 0])\n",
    "\n",
    "    axes[1, 1].plot(loss_history, label='Loss Function')\n",
    "    axes[1, 1].set_xlabel('Iterations')\n",
    "    axes[1, 1].set_ylabel('Mean Squared Error')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "\n",
    "    st.pyplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a15f6a2",
   "metadata": {},
   "source": [
    "### Function to plot the loss function with derivatives and minima/maxima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e84cd2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_function(loss_history):\n",
    "    iterations = np.arange(len(loss_history))\n",
    "    derivatives = np.gradient(loss_history)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    ax.plot(iterations, loss_history, label='Loss Function', color='blue')\n",
    "    ax.plot(iterations, derivatives, label='Derivative', color='orange')\n",
    "    global_min = np.min(loss_history)\n",
    "    global_min_index = np.argmin(loss_history)\n",
    "    global_max = np.max(loss_history)\n",
    "    global_max_index = np.argmax(loss_history)\n",
    "\n",
    "    ax.plot(global_min_index, global_min, 'go', label='Global Minima')\n",
    "    ax.plot(global_max_index, global_max, 'ro', label='Global Maxima')\n",
    "\n",
    "    ax.set_xlabel('Iterations')\n",
    "    ax.set_ylabel('Value')\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "    st.pyplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84f9a4a",
   "metadata": {},
   "source": [
    "### Streamlit Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbcd24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "st.title(\"Interactive Gradient Descent Visualization\")\n",
    "\n",
    "learning_rate = st.sidebar.slider('Learning Rate', min_value=1e-5, max_value=1e-1, value=1e-3, step=1e-5, format=\"%.5f\")\n",
    "iterations = st.sidebar.slider('Number of Iterations', min_value=100, max_value=5000, value=1000, step=100)\n",
    "initial_coefficients = np.array([3.0, 3.0, 3.0])\n",
    "\n",
    "coefficients_history, loss_history = perform_gradient_descent(learning_rate, iterations, initial_coefficients)\n",
    "plot_gradient_descent(coefficients_history, loss_history, learning_rate, iterations)\n",
    "plot_loss_function(loss_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
